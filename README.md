# LLM Chat App

A minimal chat application that connects a React frontend with a FastAPI backend and a locally hosted LLM using [llama.cpp](https://github.com/ggerganov/llama.cpp).  
Supports multiple **personas** for different chat styles.

## Features
- ðŸ”¹ React frontend (Vite + Tailwind) with dark-mode chat UI  
- ðŸ”¹ FastAPI backend for handling chat requests  
- ðŸ”¹ Local inference using llama.cpp server  
- ðŸ”¹ Predefined personas:
  1. **Concise Email Writer** â€“ writes professional and concise messages  
  2. **Explanatory Teacher** â€“ explains with metaphors and clarity  
  3. **Technical Expert** â€“ detailed technical explanations  
  4. **Normal Chatbot** â€“ casual, general-purpose chat  

## Tech Stack
- **Frontend:** React (Vite), Tailwind CSS  
- **Backend:** FastAPI, Uvicorn  
- **LLM Runtime:** llama.cpp server  
- **Model:** Qwen-3 4B (quantized)  

<img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/bdf3f207-a0b5-4b51-ad60-40d4253d0910" />

<img width="1920" height="1080" alt="image" src="https://github.com/user-attachments/assets/db38dfb0-af13-47cb-978d-c865ed5c8454" />


